install-emacs-centos.sh
------------------------------------------------------------
#!/bin/bash

echo "install emacs ..."
yum install wget gcc ncurses-devel clang git -y

if [ ! -d "/opt/software" ]
  then
    mkdir -p /opt/software
fi

cd /opt/software

wget https://mirrors.sjtug.sjtu.edu.cn/gnu/emacs/emacs-26.3.tar.xz --no-check-certificate
xz -d emacs-26.3.tar.xz  # 解压
tar -xvf emacs-26.3.tar
cd emacs-26.3  # 进入安装目录
# 配置参数 提示什么就加上什么参数
./configure --with-x-toolkit=no  --with-gif=no --with-tiff=no  --with-gnutls=no  --with-jpeg=no  --with-png=no
make && make install
# 安装完毕
# 加载常用配置
mkdir settings
git clone https://gitee.com/abrance/emacs_set.git settings
cd settings
mkdir -p ~/.emacs.d/elpa
cp --force init.el ~/.emacs.d
cp -r --force --parents elpa ~/.emacs.d
echo "install end"
------------------------------------------------------------

install-emacs-debian10.sh
------------------------------------------------------------
#!/bin/bash

echo "install emacs ..."
apt install wget gcc libncurses5-dev clang git -y

if [ ! -d "/opt/software" ]
  then
    mkdir -p /opt/software
fi

cd /opt/software

wget https://mirrors.sjtug.sjtu.edu.cn/gnu/emacs/emacs-26.3.tar.xz --no-check-certificate
xz -d emacs-26.3.tar.xz  # 解压
tar -xvf emacs-26.3.tar
cd emacs-26.3  # 进入安装目录
# 配置参数 提示什么就加上什么参数
./configure --with-x-toolkit=no  --with-gif=no --with-tiff=no  --with-gnutls=no  --with-jpeg=no  --with-png=no ./configure --with-pop
make && make install
# 安装完毕
# 加载常用配置
mkdir settings
git clone https://gitee.com/abrance/emacs_set.git settings
cd settings
mkdir -p ~/.emacs.d/elpa
cp --force init.el ~/.emacs.d
cp -r --force --parents elpa ~/.emacs.d
echo "install end"
------------------------------------------------------------


kubexxx
------------------------------------------------------------
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

cat > /usr/lib/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
[Service]
EnvironmentFile=/opt/kubernetes/cfg/kube-controller-manager.conf
ExecStart=/opt/kubernetes/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF

https://codeload.github.com/cilium/cilium/tar.gz/v1.8.4


{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "192.192.100.73",
   }
}

apiVersion: v1
kind: Node
metadata:
  name: 192.192.100.73
  labels:
    name: k8s-node1

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
  repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
------------------------------------------------------------


单机部署 cilium1.8 大概流程
------------------------------------------------------------
kubeadm init \
--image-repository registry.aliyuncs.com/google_containers \
--service-cidr=10.96.0.0/12 \
--pod-network-cidr="10.244.0.0/16" \
--apiserver-advertise-address 192.192.100.79 \
--control-plane-endpoint 192.192.100.79 \
--token-ttl 0

> 容忍度
      tolerations:
      - operator: Exists

> kubectl edit

> kubectl port-forward -n kube-system svc/hubble-ui --address 0.0.0.0 --address :: 12000:80


> kubeadm 搭建集群排错
	>> kubeadm init 后发现 在 报错 kubelet check timeout ，查看日志发现原因是 etcd 没有启动，原因为 不能分配 10.xxx 的地址，当修改了 IP 地址和范围后可用

------------------------------------------------------------


curl 
------------------------------------------------------------
> tar zxvf curl-7.83.1.tar.gz
> cd curl-7.83.1/
> ./configure --with-ssl
> make && make install
------------------------------------------------------------


istio
************************************************************

------------------------------------------------------------
> istio 组件
	>> citadel 组件
		>>> caserver
		>>> scret controller
		>>> keycertRotator
	>> pilot 组件
		>>> pilot-discovery
		>>> pilot-agent
	>> 注册中心
		>>> 
------------------------------------------------------------

------------------------------------------------------------
> Citadel
	>> 网格内唯一的身份管理组件，主要负责为集群内的服务账户
		颁发证书
		启动 gRPC 服务处理证书签名请求 CSR 
		根证书轮换。因此 Citadel 主要包含 Secret Controller、gRPC服务器及证书轮换器等组件。在默认情况下，证书轮换器不启动，它主要是为多集群CA证书自动轮转使用的

	>> how
		>>> IstioCA object
			用于签发证书的对象，实现了CertificateAuthority接口。目前可以使用自签名证书或者命令行指定的证书作为证书授权中心的证书
		>>> Secret Controller   维护 K8s service account controller 和 Secret controller 的关系
			初始化并运行 SecretController，以 secret 方式保存证书密钥对
		>>> gRPC Server
			以grpc 方式提供签发证书服务（认证后签发）
		>>> KeyCertBundleRotator 证书轮换器
			将证书进行管理，过期轮换等（高可用设计：目的：更换证书不导致服务中断 实现： KeyCertRetriever watch KeyCertBundle, KeyCertBundle到了轮换时期更新管理的证书私钥，这样将不需要重启服务）
		>>> (SDS server)
			虚拟机、物理机场景下，不能使用 Secret 挂载 pod 方式为 envoy 提供证书密钥对。Istio 将 Node agent 部署到 envoy 主机中，需要请求证书时，将请求 CAServer ，并将证书保存于本地 /etc/certs 中供 envoy 安全认证使用。由 Node Agent 轮换证书。
	>> 

> k8s Secret
	>> secret用来保存小片敏感数据的k8s资源，例如密码，token，或者秘钥。这类数据当然也可以存放在Pod或者镜像中，但是放在Secret中是为了更方便的控制如何使用数据，并减少暴露的风险。

> Pilot-agent
	>> 概念
		核心功能主要包括Envoy代理的启动、热重启、生命周期守护及优雅退出
		>>> envoy 启动
			bootstrap 配置文件生成
		>>> envoy 热重启
			Sidecar 中，证书默认在 容器的文件系统中，以 k8s Secret 卷方式挂载，当 Secret 发生变化，k8s 重新挂载，envoy 发生热重启。
			wathcer 负责监听证书文件
			1 证书文件更新，通知 watcher
			2 watcher 计算证书文件哈希值，将哈希值发送到配置队列中
			3 Proxy agent 收到配置更新，计算hash一致，不一致则启动新的 envoy 进程
			4 新 envoy 和 老 envoy 完成交替
		>>> 守护 envoy
			类似 systemd
		>>> 优雅退出

> pilot-discovery
	>> what
		pilot-discovery 实现了 Pilot 组件，为Envoy提供Listener、Route、Cluster和Endpoint配置。Pilot在运行时对外提供gRPC服务，在所有 Envoy 代理和 pilot 直接都建立一条 gRPC长连接，并订阅 xDS配置。
	>> how
		>>> 初始化配置
			>>>> 日志
			>>>> SPIFFE Trust Domain
			>>>> PilotServer
				>>>>> config Controller
					主要用于监听 Kube-apiserver 中的配置资源，在内存中缓存监听到的所有配置资源，并在更新Config资源时调用注册的事件处理函数
					Config资源都是通过CRD定义并保存在 Kubernetes 中的，所以ConfigController 上实际是一个 CRD控制器，它从 Kubernetes 平台监听所有的IstioConfigTypes。从IstioConfigTypes定义中可以了解Istio所有的Config资源类型，主要涉及网络配置、认证、鉴权、策略管理等
				>>>>> Service Controller

					
				>>>>> istioConfigStore

------------------------------------------------------------


------------------------------------------------------------
> 安装 istio
	>> 安装 k8s
	>> 


> 安全
  密钥和证书管理
	数据面 节点间安全通信
	安全相关配置的分发
	授权和访问审计

> istio 安全组件
 安全核心组件Citadel用于密钥和证书管理；
 Envoy作为数据面组件代理服务间的安全通信，包括认证、通道加密等；
 Pilot作为配置管理服务，在安全场景下将安全相关的配置分发给Envoy；
 Mixer可以通过配置Adapter来做授权和访问审计。

> 在部署和配置阶段关注安全相关的从控制面到数据面的配置流程
	Citadel监听apiserver，为每个Service都生成密钥和证书，并保存为Kubernetes Secrets；
	当创建Pod时，Kubernetes将包含密钥和证书的Secret挂载到对应的Pod中；
	Citadel会维护证书的生命周期，并根据配置定期重建Kubernetes Secrets以自动更新证书；
	Pilot生成配置信息，定义哪个Service Account可以运行哪个服务，并将这个配置下发给Envoy

> 认证和授权
	>> 认证
		传输认证
			通过证书进行认证 SAN
		来源认证
			JWT 
	
> ISTIO 预研
	>> NG
	>> inbound outbound
	>> ingress egress
	>> 认证 鉴权

------------------------------------------------------------
************************************************************

> cilium 1.8 单点部署于 k8s 1.20.8
	>> 环境
		一台 centos7 设备
	>> 环境准备
		>>> docker 20.10.17  修改 /etc/docker/daemon.json 后 systemctl reload && systemctl restart docker
------------------------------------------------------------
{
  "registry-mirrors" : [
    "http://registry.docker-cn.com",
    "http://docker.mirrors.ustc.edu.cn",
    "http://hub-mirror.c.163.com"
  ],
  "insecure-registries" : [
    "registry.docker-cn.com",
    "docker.mirrors.ustc.edu.cn"
  ],
  "debug" : true,
  "experimental" : true
}
------------------------------------------------------------
		>>> kubeadm 等 k8s 组件
------------------------------------------------------------
yum install -y yum-utils
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

yum install kubeadm-1.20.8 kubelet-1.20.8
yum --enablerepo=elrepo-kernel install kernel-ml
------------------------------------------------------------
		>>> 升级 linux 内核
			https://www.jianshu.com/p/fdf6bb6c5b9c
	>> 安装
		>>> k8s 1.20.8 部署
------------------------------------------------------------
kubeadm init \
--image-repository registry.aliyuncs.com/google_containers \
--service-cidr=10.96.0.0/12 \
--pod-network-cidr="10.244.0.0/16" \
--apiserver-advertise-address ${IP} \
--control-plane-endpoint ${IP} \
--token-ttl 0
# 失败了必须 kubeadm reset
------------------------------------------------------------
		>>> helm3 安装
------------------------------------------------------------

------------------------------------------------------------


------------------------------------------------------------
>> istio 预研
	>>> pilot 生成 xds 的方式 异同
	>>> 安全方面的代码

	>>> 目标
		不想用 sidecar （每个节点布置一个 service）
		想用网关模式（只有 router 的节点布置 service）
	>>> point
		VS GW DR SE

>> inbound xds 的生成： 应该是原始目的地类型 
	>>> how 如何生成的
------------------------------------------------------------


> k8s 网络解决
	>> 容器与容器
	>> 抽象的 POD 和 POD 之间的通信
	>> POD 到 Service 之间的通信
	>> 集群内部与外部组件之间的通信

> k8s CNI 原理
	>> 概念
		CNI  Container Network Interface   CNM Container Network Model  两种容器网络规范。 CNI 被更多公司接受
		CNI提供了一种应用容器的插件化网络解决方案，定义对容器网络进行操作和配置的规范，通过插件的形式对CNI接口进行实现。CNI是由rkt Networking Proposal发展而来的，尝试提供一种普适的容器网络解决方案。CNI仅关注在创建容器时分配网络资源与在销毁容器时删除网络资源，这使得CNI规范非常轻巧、易于实现，得到了广泛的支持
		>>> 模型概念
			>>>> 容器
				是拥有独立Linux网络命名空间的环境，例如使用Docker或rkt创建的容器。关键之处是容器需要拥有自己的Linux网络命名空间，这是加入网络的必要条件
			>>>> 网络
				表示可以互连的一组实体，这些实体拥有各自独立、唯一的IP地址，可以是容器、物理机或者其他网络设备（比如路由器）等。可以将容器添加到一个或多个网络中，也可以从一个或多个网络中删除
	>> why
	>> how

> rDS 获取失败  bug


> xDS server 原理
------------------------------------------------------------
   ┌────────┐         ┌───────┐
   │ envoy  │         │ server│
   └───┬────┘         └───┬───┘
       │                  │
       │     Request  1   │
       ├────────────────► │
       │                  │
       │     Response  2  │
       │ ◄────────────────┤
       │                  │
       │     ACK/NACK  3  │
       ├────────────────► │
       │                  │
       │                  │

1 envoy send discovery request to server

2 server recieve the request and send response to envoy

3 envoy recieve the response , dynamic load  a  part of config, according to the
  part, run ACK/NACK .
------------------------------------------------------------
	
> xDS request response
	>> 概念
		xDS 可以有两种方式承载，gRPC 和 REST，这两种方式都通过 xDS-API 发送 discovery request ，资源通过 discovery request 下发
	>> 
		l

> xDS 传输协议
	https://rocdu.gitbook.io/deep-understanding-of-istio/9/0
	sotw 和 增量传输


> ss
	>> 概念
		ss 用于获取 socket 统计信息，ss socket statistics，优势在于能够显示更多更详细的 TCP 连接状态信息，比 netstat 更快
		ss [参数] [过滤]
	>> 常用选项
		-l  display listening sockets
		-a  display all sockets
		-p  show process using socket 显示进程
		-t --tcp  display only tcp socket
		-u --udp  display only udp socket
		-x --unix  display only unix socket
		-o --options  show more information

	>> e.g.
		ss -tlpn -o state established '( dport = 6443 or sport = 6443 )'  查看建立的tcp连接，端口为 6443 的socket
		ss -pl dst 127.0.0.1:6443  查看目的地址为 6443 的 socket
		ss -pl src *:6443  查看源端口为6443的socket
		
> date
	>> 显示时间命令
	>> 技巧
		时间格式化
		date +"%Y-%m-%d %H:%M:%s"  可以拿到格式化数据，并存入到变量中


> golang语言
	>> 中级
		>>> new make 的区别
			new 和 make 是两个内置函数，主要用来创建并分配类型的内存 
			>>>> 使用场景
				make 只能用于 chan、map、slice 类型
				new 可以适用任何类型
			>>>> 返回
				make 返回的是引用类型 ，因为chan map slice 本身就是引用类型
				new 返回的是类型的指针
			>>>> 分配空间
				make 分配间后会进行初始化 , 因为chan map slice 是引用类型，所以必须初始化
				new 分配的空间被清零 

		>>> 空 struct{} 用途
			struct{} 没有元素，size 为0，通常在没有信息存储时使用。节省内存，表示这里不需要一个值
			如实现方法接收对象、集合、空通道  set[value] = struct{}{}  chan <- struct{}{} 
			
		>>> slice 的理解
			>>>> 内部结构
				slice 有三个元素构成：array 指针、len、cap。array 表示真实存数据的地址，len表示 slice 的元素个数，cap表示slice 指向的底层数组容量。array存储于连续内存所以随机访问很快，适合下标访问，缓存命中率很高；动态扩容会涉及内存拷贝和开辟新内存，会带来gc压力，内存碎片化；如果可预估使用空间，提前分配cap的大小是极好的；
				扩容策略：
					当所需容量 cap 大于原先容量 cap 的 2 倍，则最终申请 cap 容量为当前所需容量
					当所需要容量 cap 不大于原容量的 2 倍，slice len 小于 1024 则增长因子为 1 （3 变 6个） 大于 1024 则增长因子为 0.25 （增原本容量的 1/4）
				扩容内存策略：
					非指针类型扩容将直接向后申请连续空间扩容
				根据最后计算的 size 还需要根据 slice 的类型进行 内存对齐
			>>>> 并发安全
				并发写不安全
				
		>>> channel 的理解
			>>>> 内部结构
				
			>>>> 并发安全
				是并发安全的
			
		>>> map 的理解
			>>>> 并发写会 panic
			
		>>> interface 的理解
		>>> select 的理解
		>>> atomic 的理解
		>>> 协程泄漏		
		>>> 并发模型的设计和理解
	
	>> 高级
		>>> Go 语言GC(垃圾回收)的工作原理 （有哪些，原理，和为什么要使用这个，优化）
		>>> gmp 模型


> OS
	>> 进程 线程 协程 是什么？各自的优势？
		>>> 定义
			进程是操作系统资源分配最小单位
			线程是CPU调度最小单位
			协程是用户态的轻量级线程
		>>> 资源占用
			进程拥有独立的 虚拟内存 文件描述符 栈 寄存器（不共享堆，不共享栈）
			线程拥有 栈 寄存器 程序计数器（共享堆，不共享栈）
			协程拥有自己的寄存器 栈
		>>> 互相的关系
			线程依赖进程，进程至少有一个线程，同一进程下的线程共享内存
			协程最终依赖线程执行程序，比起线程，少了很多上下文切换
		>>> 使用场景
			多进程处理 CPU 密集型任务，如计算
			多线程处理 IO 密集型任务，如读写文件
			协程本身依赖线程，所以适合IO 密集型任务，而协程上下文切换非常小，所以一般来说优于多线程
		>>> 利用多核资源
			多进程可以利用多核
			多线程可以利用多核
			协程不可以利用多核
		>>> 调度
			协程不受内核调度，进程线程需要内核调度
			协程是异步机制，进程线程是同步机制


	>> 有哪些 IO 模型？各自的优势？golang 的网络模型用的哪个？
		>>> 阻塞 | 非阻塞 IO
			>>>> 当调用 API 时，如果 API 没有准备好数据一直不返回，则为阻塞 IO
			>>>> 当没有准备好数据时直接返回一个 Block 错误，告诉应用状态，则为非阻塞 IO
		>>> 同步 | 异步 IO
			>>>> 当数据一直没有准备好，直到检测到他已经准备好，再进行读操作，为 同步 IO
			>>>> 当数据没有准备好时，应用向系统直接注册一个准备好后进行读取操作的事件，为 异步 IO
		>>> IO 多路复用
			>>>> 解决什么问题？
				做到资源的最大利用： 加入了一个中间层来管理 连接，而不是每个socket马上起一个线程连接（python httpsever就是这么干的）
			>>>> 定义
				数据通信系统或计算机网络系统中，传输媒体的带宽或容量往往会大于传输单一信号的需求，为了有效地利用通信线路,希望一个信道同时传输多路信号，这就是所谓的多路复用技术
				所以，多路指多路的信号，复用指信道的复用。在 网络io 多路复用中，多路指 多个socket，复用指，一个程序（线程）的复用
				IO多路复用在Linux下包括了三种，select、poll、epoll，抽象来看，他们功能是类似的，但具体细节各有不同：首先都会对一组文件描述符进行相关事件的注册，然后阻塞等待某些事件的发生或等待超时。IO多路复用都可以关注多个文件描述符，但对于这三种机制而言，不同数量级文件描述符对性能的影响是不同的
				I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间
			>>>> select
				使用copy_from_user从用户空间拷贝fd_set到内核空间
				注册回调函数__pollwait
				遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）
				以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。
				__pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。
				poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。
				如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。
				把fd_set从内核空间拷贝到用户空间。
				>>>>> 缺点
					每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
					同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
					select支持的文件描述符数量太小了，默认是1024
			>>>> poll
				poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多
			>>>> epoll
				epoll既然是对select和poll的改进，就应该能避免上述的三个缺点
				>>>>> 解决
					在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。
					>>>>>> 第一个缺点
						epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次
					>>>>>> 第二个缺点
						epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）
					>>>>>> 第三个缺点
						epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大
		
		
	>> 进程间通信方式有哪些？
		>>> socket
		>>> 匿名管道
		>>> 有名管道（FIFO）
		>>> 信号
		>>> 消息队列
		>>> 共享内存
		>>> 信号量

> Linux
	>> 常用的网络调试工具用过哪些？
		ping
		iproute
		net-tools 		(arp, ifconfig)
		arping
		tcpdump wireshark
		top
		ss
		netstat
		curl
		wget
		netcat
		openssl
		traceroute
		telnet
		lsof
		strace
			
	>> 常用的编辑器有哪些？如何搜索、翻页、撤回？
		>>> vim  搜索 / , ? ; 翻页 PgDn C-f , PgUp C-b,  撤回 u;
		>>> nano  搜索 C-w,  M-w ; 翻页 C-y C-v, 撤回 M-u;
		>>> emacs  搜索 C-s C-r; 翻页 C-v , M-v;撤回 C-/;
		>>> grep ""  搜索
		>>> sed -n '/${RE}/p'

> 网络
	>> 详细说明一个客户端访问 https://www.baidu.com/ 的过程
	>> 定位到一个 http 客户端请求失败如何进行调试？


> 业务逻辑
	>> 内存数据和磁盘数据数据一致性问题

> 附加题
	>> k8s 相关
		>>> k8s 解决什么问题？
		>>> k8s 各个组件的理解？
		>>> k8s 网络组件解决什么问题，用到哪些技术？


